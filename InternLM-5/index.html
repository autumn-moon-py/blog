<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blog.subrecovery.top","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Imdepoly本地部署使用">
<meta property="og:type" content="article">
<meta property="og:title" content="书生·浦语大模型实战营五">
<meta property="og:url" content="https://blog.subrecovery.top/InternLM-5/index.html">
<meta property="og:site_name" content="秋月的博客">
<meta property="og:description" content="Imdepoly本地部署使用">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.subrecovery.top/InternLM-5/image-20240409143847482.png">
<meta property="og:image" content="https://blog.subrecovery.top/InternLM-5/image-20240409143820326.png">
<meta property="og:image" content="https://blog.subrecovery.top/InternLM-5/image-20240409150152351.png">
<meta property="og:image" content="https://blog.subrecovery.top/InternLM-5/image-20240409144518219.png">
<meta property="og:image" content="https://blog.subrecovery.top/InternLM-5/image-20240409152142549.png">
<meta property="og:image" content="https://blog.subrecovery.top/InternLM-5/image-20240409154515002.png">
<meta property="og:image" content="https://blog.subrecovery.top/InternLM-5/image-20240409154722349.png">
<meta property="og:image" content="https://blog.subrecovery.top/InternLM-5/image-20240409154855223.png">
<meta property="og:image" content="https://blog.subrecovery.top/InternLM-5/image-20240409155520338.png">
<meta property="article:published_time" content="2024-04-09T06:25:22.000Z">
<meta property="article:modified_time" content="2024-04-09T13:41:43.275Z">
<meta property="article:author" content="秋月">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.subrecovery.top/InternLM-5/image-20240409143847482.png">

<link rel="canonical" href="https://blog.subrecovery.top/InternLM-5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>书生·浦语大模型实战营五 | 秋月的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="秋月的博客" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">秋月的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://blog.subrecovery.top/InternLM-5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="秋月">
      <meta itemprop="description" content="记录自己的学习经验">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="秋月的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          书生·浦语大模型实战营五
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2024-04-09 14:25:22 / 修改时间：21:41:43" itemprop="dateCreated datePublished" datetime="2024-04-09T14:25:22+08:00">2024-04-09</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>Imdepoly本地部署使用</p>
<span id="more"></span>

<h1 id="初体验"><a href="#初体验" class="headerlink" title="初体验"></a>初体验</h1><p><img src="/InternLM-5/image-20240409143847482.png" alt="image-20240409143847482"></p>
<h1 id="速度对比"><a href="#速度对比" class="headerlink" title="速度对比"></a>速度对比</h1><p><img src="/InternLM-5/image-20240409143820326.png" alt="image-20240409143820326"></p>
<p>lmdeploy</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">start = time.time()</span><br><span class="line">pipe = lmdeploy.pipeline(model_path)</span><br><span class="line">response = pipe([&quot;你好&quot;, &quot;请提供三条关于时间管理的建议&quot;])</span><br><span class="line">final = time.time() - start</span><br><span class="line">print(f&quot;耗时&#123;final&#125;&quot;)</span><br><span class="line">print(response)</span><br><span class="line">print(&quot;----------------------------------------------------&quot;)</span><br><span class="line"></span><br><span class="line">耗时4.675809383392334</span><br><span class="line">[Response(text=&#x27;你好，我可以帮助你。你想了解什么？&#x27;, generate_token_len=10, session_id=0, finish_reason=&#x27;stop&#x27;), Response(text=&#x27;当然 ，以下是三条关于时间管理的建议：\n\n1. **设定明确的目标和优先级**：\n   - 确定每天或每周的长期目标，并根据其重要性和紧急程度进行排序 。\n   - 优先处理那些对实现长期目标至关重要的任务。\n\n2. **使用时间管理工具**：\n   - 利用日历、待办事项清单或时间跟踪应用程序来帮助规划和跟踪时间。\n   - 根据不同任务和活动分配特定的时间段，确保它们得到充分的关注。\n\n3. **学会说“不”**：\n   - 认识到自己的时间有限，学会拒绝那些不重要或无法立即完成的任务。\n   - 避免过度承诺，以免给自己带来过多的压力。\n   - 保持灵活性，以便能够适应突发事件或新 的任务。\n\n这些建议旨在帮助您更好地管理时间，从而提高效率、减少压力，并实现个人和职业目标。&#x27;, generate_token_len=196, session_id=1, finish_reason=&#x27;stop&#x27;)]</span><br></pre></td></tr></table></figure>

<p>Transformer</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">start = time.time()</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, trust_remote_code=True).cuda()</span><br><span class="line">model = model.eval()</span><br><span class="line"></span><br><span class="line">inp = &quot;你好&quot;</span><br><span class="line">print(&quot;[INPUT]&quot;, inp)</span><br><span class="line">response, history = model.chat(tokenizer, inp, history=[])</span><br><span class="line">print(&quot;[OUTPUT]&quot;, response)</span><br><span class="line"></span><br><span class="line">inp = &quot;请提供三条关于时间管理的建议&quot;</span><br><span class="line">print(&quot;[INPUT]&quot;, inp)</span><br><span class="line">response, history = model.chat(tokenizer, inp, history=history)</span><br><span class="line">print(&quot;[OUTPUT]&quot;, response)</span><br><span class="line"></span><br><span class="line">final = time.time() - start</span><br><span class="line">print(f&quot;耗时&#123;final&#125;&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[INPUT] 你好</span><br><span class="line">[OUTPUT] 你好！很高兴能和你交流。请问有什么我可以帮助你的吗？</span><br><span class="line">[INPUT] 请提供三条关于时间管理的建议</span><br><span class="line">[OUTPUT] 当然可以，以下是关于时间管理的建议：</span><br><span class="line"></span><br><span class="line">1. 制定明确的目标和计划：在开始任何任务之前，先确定自己的目标和计划，这样可以避免浪费时间和精力在不必要的任务上。</span><br><span class="line">2. 优先处理重要事项：将任务按照重要性和紧急程度排序，先处理最重要的任务，这样可以确保时间和精力都用在最有价值的事情上。</span><br><span class="line">3. 学会拒绝：有时候，我们可能会被各种干扰和诱惑所困扰，导致无法集中精力完成任务。学会拒绝那些不重要或不紧急的事情，可以帮助我们更好地管理时间。</span><br><span class="line">耗时24.595902681350708</span><br></pre></td></tr></table></figure>

<h1 id="终端聊天"><a href="#终端聊天" class="headerlink" title="终端聊天"></a>终端聊天</h1><p>turbomind后是模型路径，我直接放相对本地路径了</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lmdeploy chat turbomind internlm2-chat-1_8b</span><br></pre></td></tr></table></figure>

<p><img src="/InternLM-5/image-20240409150152351.png" alt="image-20240409150152351"></p>
<p>不知道为什么torch跑不了，tritonclient也装了</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">D:\project\ai_chat\lmdeploy&gt;cmd /k &quot;conda activate lmdeploy&amp;&amp;lmdeploy chat torch internlm2-chat-1_8b&quot;</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\lib\runpy.py&quot;, line 196, in _run_module_as_main</span><br><span class="line">    return _run_code(code, main_globals, None,</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\lib\runpy.py&quot;, line 86, in _run_code</span><br><span class="line">    exec(code, run_globals)</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\Scripts\lmdeploy.exe\__main__.py&quot;, line 7, in &lt;module&gt;</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\lib\site-packages\lmdeploy\cli\entrypoint.py&quot;, line 18, in run</span><br><span class="line">    args.run(args)</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\lib\site-packages\lmdeploy\cli\chat.py&quot;, line 80, in torch</span><br><span class="line">    run_chat(args.model_path,</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\lib\site-packages\lmdeploy\pytorch\chat.py&quot;, line 65, in run_chat</span><br><span class="line">    from lmdeploy.pytorch.engine import Engine</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\lib\site-packages\lmdeploy\pytorch\engine\__init__.py&quot;, line 2, in &lt;module&gt;</span><br><span class="line">    from .engine import Engine</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\lib\site-packages\lmdeploy\pytorch\engine\engine.py&quot;, line 22, in &lt;module&gt;</span><br><span class="line">    from .model_agent import AutoModelAgent, ModelInputs</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\lib\site-packages\lmdeploy\pytorch\engine\model_agent.py&quot;, line 18, in &lt;module&gt;</span><br><span class="line">    from ..models import patch</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\lib\site-packages\lmdeploy\pytorch\models\__init__.py&quot;, line 2, in &lt;module&gt;</span><br><span class="line">    from .patch import patch</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\lib\site-packages\lmdeploy\pytorch\models\patch.py&quot;, line 14, in &lt;module&gt;</span><br><span class="line">    from ..dist_utils import partition_module, replicate_module</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\lib\site-packages\lmdeploy\pytorch\dist_utils.py&quot;, line 9, in &lt;module&gt;</span><br><span class="line">    from lmdeploy.pytorch.models.q_modules import QLinear</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\lib\site-packages\lmdeploy\pytorch\models\q_modules.py&quot;, line 7, in &lt;module&gt;</span><br><span class="line">    from ..kernels.w8a8_triton_kernels import (matmul_kernel_dynamic_quant,</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\lib\site-packages\lmdeploy\pytorch\kernels\__init__.py&quot;, line 2, in &lt;module&gt;</span><br><span class="line">    from .alibi_pagedattention import alibi_paged_attention_fwd</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\lib\site-packages\lmdeploy\pytorch\kernels\alibi_pagedattention.py&quot;, line 6, in &lt;module&gt;</span><br><span class="line">    import triton</span><br><span class="line">ModuleNotFoundError: No module named &#x27;triton&#x27;</span><br></pre></td></tr></table></figure>



<h1 id="启动api服务器"><a href="#启动api服务器" class="headerlink" title="启动api服务器"></a>启动api服务器</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import subprocess</span><br><span class="line"></span><br><span class="line">#下面模型路径自己改</span><br><span class="line">command = &quot;lmdeploy serve api_server \</span><br><span class="line">    D:\\project\\ai_chat\\models\\internlm2-chat-1_8b \</span><br><span class="line">    --model-format hf \</span><br><span class="line">    --quant-policy 0 \</span><br><span class="line">    --server-name 127.0.0.1 \</span><br><span class="line">    --server-port 23333 \</span><br><span class="line">    --tp 1&quot;</span><br><span class="line">subprocess.run(command, shell=True)</span><br></pre></td></tr></table></figure>

<p><img src="/InternLM-5/image-20240409144518219.png" alt="image-20240409144518219"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">url = &quot;http://127.0.0.1:23333/v1/chat/completions&quot;</span><br><span class="line">payload = &#123;</span><br><span class="line">    </span><br><span class="line">    &quot;model&quot;: &quot;internlm2-chat-1_8b&quot;,</span><br><span class="line">  &quot;messages&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;content&quot;: &quot;你好&quot;,</span><br><span class="line">      &quot;role&quot;: &quot;user&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  &quot;temperature&quot;: 0.7,</span><br><span class="line">  &quot;top_p&quot;: 1,</span><br><span class="line">  &quot;n&quot;: 1,</span><br><span class="line">  &quot;max_tokens&quot;: 512,</span><br><span class="line">  &quot;stop&quot;: None,</span><br><span class="line">  &quot;stream&quot;: False,</span><br><span class="line">  &quot;presence_penalty&quot;: 0,</span><br><span class="line">  &quot;frequency_penalty&quot;: 0,</span><br><span class="line">  &quot;user&quot;: &quot;string&quot;,</span><br><span class="line">  &quot;repetition_penalty&quot;: 1,</span><br><span class="line">  &quot;session_id&quot;: -1,</span><br><span class="line">  &quot;ignore_eos&quot;: False</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    &#x27;accept&#x27;: &#x27;application/json&#x27;,</span><br><span class="line">    &#x27;Content-Type&#x27;: &#x27;application/json&#x27;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.request(&quot;POST&quot;, url, headers=headers, data=json.dumps(payload))</span><br><span class="line">print(response.text)</span><br><span class="line"></span><br><span class="line">&#123;&quot;id&quot;:&quot;5724&quot;,&quot;object&quot;:&quot;chat.completion&quot;,&quot;created&quot;:1712645525,&quot;model&quot;:&quot;internlm2-chat-1_8b&quot;,&quot;choices&quot;:[&#123;&quot;index&quot;:0,&quot;message&quot;:&#123;&quot;role&quot;:&quot;assistant&quot;,&quot;content&quot;:&quot;你 好，有什么我可以帮助你的吗？&quot;&#125;,&quot;finish_reason&quot;:&quot;stop&quot;&#125;],&quot;usage&quot;:&#123;&quot;prompt_tokens&quot;:103,&quot;total_tokens&quot;:112,&quot;completion_tokens&quot;:9&#125;&#125;</span><br></pre></td></tr></table></figure>

<h1 id="网页聊天"><a href="#网页聊天" class="headerlink" title="网页聊天"></a>网页聊天</h1><p>启动失败</p>
<h2 id="api"><a href="#api" class="headerlink" title="api"></a>api</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lmdeploy serve api_server \</span><br><span class="line">    D:\\project\\ai_chat\\models\\internlm2-chat-1_8b \</span><br><span class="line">    --model-format hf \</span><br><span class="line">    --quant-policy 0 \</span><br><span class="line">    --server-name 127.0.0.1 \</span><br><span class="line">    --server-port 23333 \</span><br><span class="line">    --tp 1</span><br></pre></td></tr></table></figure>

<p>控制台</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">model_source: hf_model</span><br><span class="line">Device does not support bf16.</span><br><span class="line">model_config:</span><br><span class="line"></span><br><span class="line">[llama]</span><br><span class="line">model_name = internlm2-chat-1_8b</span><br><span class="line">tensor_para_size = 1</span><br><span class="line">head_num = 16</span><br><span class="line">kv_head_num = 8</span><br><span class="line">vocab_size = 92544</span><br><span class="line">num_layer = 24</span><br><span class="line">inter_size = 8192</span><br><span class="line">norm_eps = 1e-05</span><br><span class="line">attn_bias = 0</span><br><span class="line">start_id = 1</span><br><span class="line">end_id = 2</span><br><span class="line">session_len = 32776</span><br><span class="line">weight_type = fp16</span><br><span class="line">rotary_embedding = 128</span><br><span class="line">rope_theta = 1000000.0</span><br><span class="line">size_per_head = 128</span><br><span class="line">group_size = 0</span><br><span class="line">max_batch_size = 128</span><br><span class="line">max_context_token_num = 1</span><br><span class="line">step_length = 1</span><br><span class="line">cache_max_entry_count = 0.8</span><br><span class="line">cache_block_seq_len = 128</span><br><span class="line">cache_chunk_size = -1</span><br><span class="line">num_tokens_per_iter = 0</span><br><span class="line">max_prefill_iters = 1</span><br><span class="line">extra_tokens_per_iter = 0</span><br><span class="line">use_context_fmha = 1</span><br><span class="line">quant_policy = 0</span><br><span class="line">max_position_embeddings = 32768</span><br><span class="line">rope_scaling_factor = 0.0</span><br><span class="line">use_logn_attn = 0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">get 195 model params</span><br><span class="line">Input chat template with model_name is None. Forcing to use internlm2-chat-1_8b</span><br><span class="line">[WARNING] gemm_config.in is not found; using default GEMM algo</span><br><span class="line">HINT:    Please open http://127.0.0.1:23333 in a browser for detailed api usage!!!</span><br><span class="line">HINT:    Please open http://127.0.0.1:23333 in a browser for detailed api usage!!!</span><br><span class="line">HINT:    Please open http://127.0.0.1:23333 in a browser for detailed api usage!!!</span><br><span class="line">INFO:     Started server process [19684]</span><br><span class="line">INFO:     Waiting for application startup.</span><br><span class="line">INFO:     Application startup complete.</span><br><span class="line">INFO:     Uvicorn running on http://127.0.0.1:23333 (Press CTRL+C to quit)</span><br></pre></td></tr></table></figure>

<h2 id="web"><a href="#web" class="headerlink" title="web"></a>web</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lmdeploy serve gradio http://127.0.0.1:23333 \</span><br><span class="line">    --server-name 127.0.0.1 \</span><br><span class="line">    --server-port 6006</span><br></pre></td></tr></table></figure>

<p>控制台</p>
<p>gradio是3.38，估计版本问题，不过也就个网页而已，跑不了我自己也会调用api</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">D:\develop\miniconda\envs\lmdeploy\lib\site-packages\gradio_client\documentation.py:103: UserWarning: Could not get documentation group for &lt;class &#x27;gradio.mix.Parallel&#x27;&gt;: No known documentation group for module &#x27;gradio.mix&#x27;</span><br><span class="line">  warnings.warn(f&quot;Could not get documentation group for &#123;cls&#125;: &#123;exc&#125;&quot;)</span><br><span class="line">D:\develop\miniconda\envs\lmdeploy\lib\site-packages\gradio_client\documentation.py:103: UserWarning: Could not get documentation group for &lt;class &#x27;gradio.mix.Series&#x27;&gt;: No known documentation group for module &#x27;gradio.mix&#x27;</span><br><span class="line">  warnings.warn(f&quot;Could not get documentation group for &#123;cls&#125;: &#123;exc&#125;&quot;)</span><br><span class="line">server is gonna mount on: http://127.0.0.1:6006</span><br><span class="line">Running on local URL:  http://127.0.0.1:6006</span><br><span class="line"></span><br><span class="line">Could not create share link. Missing file: D:\develop\miniconda\envs\lmdeploy\lib\site-packages\gradio\frpc_windows_amd64_v0.2.</span><br><span class="line"></span><br><span class="line">Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps:</span><br><span class="line"></span><br><span class="line">1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_windows_amd64.exe</span><br><span class="line">2. Rename the downloaded file to: frpc_windows_amd64_v0.2</span><br><span class="line">3. Move the file to this location: D:\develop\miniconda\envs\lmdeploy\lib\site-packages\gradio</span><br></pre></td></tr></table></figure>

<p><img src="/InternLM-5/image-20240409152142549.png" alt="image-20240409152142549"></p>
<h1 id="进阶作业"><a href="#进阶作业" class="headerlink" title="进阶作业"></a>进阶作业</h1><h2 id="一"><a href="#一" class="headerlink" title="一"></a>一</h2><p>设置KV Cache最大占用比例为0.4，开启W4A16量化，以命令行方式与模型对话</p>
<h3 id="量化模型"><a href="#量化模型" class="headerlink" title="量化模型"></a>量化模型</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lmdeploy lite auto_awq internlm2-chat-1_8b --work-dir internlm2-chat-1_8b-4bit</span><br></pre></td></tr></table></figure>

<p><img src="/InternLM-5/image-20240409154515002.png" alt="image-20240409154515002"></p>
<h3 id="终端运行量化模型"><a href="#终端运行量化模型" class="headerlink" title="终端运行量化模型"></a>终端运行量化模型</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lmdeploy chat turbomind internlm2-chat-1_8b-4bit --model-format awq --cache-max-entry-count 0.4</span><br></pre></td></tr></table></figure>

<p><img src="/InternLM-5/image-20240409154722349.png" alt="image-20240409154722349"></p>
<h2 id="二"><a href="#二" class="headerlink" title="二"></a>二</h2><p>以API Server方式启动 lmdeploy，开启 W4A16量化，调整KV Cache的占用比例为0.4，分别使用命令行客户端与Gradio网页客户端与模型对话</p>
<p><strong>忽略之前的启动网页服务失败</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lmdeploy serve gradio internlm2-chat-1_8b-4bit --server-name 127.0.0.1 --server-port 7880 --model-format awq --cache-max-entry-count 0.4</span><br></pre></td></tr></table></figure>

<p><img src="/InternLM-5/image-20240409154855223.png" alt="image-20240409154855223"></p>
<h2 id="三"><a href="#三" class="headerlink" title="三"></a>三</h2><p>使用W4A16量化，调整KV Cache的占用比例为0.4，使用Python代码集成的方式运行internlm2-chat-1.8b模型</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from lmdeploy import pipeline, TurbomindEngineConfig</span><br><span class="line"></span><br><span class="line">model_path = &quot;internlm2-chat-1_8b-4bit&quot;</span><br><span class="line"></span><br><span class="line">backend_config = TurbomindEngineConfig(cache_max_entry_count=0.4)</span><br><span class="line">pipe = pipeline(model_path,backend_config=backend_config)</span><br><span class="line">response = pipe([&quot;你好&quot;])</span><br><span class="line">print(response)</span><br></pre></td></tr></table></figure>

<p><img src="/InternLM-5/image-20240409155520338.png" alt="image-20240409155520338"></p>
<h2 id="四"><a href="#四" class="headerlink" title="四"></a>四</h2><p>使用 LMDeploy 运行视觉多模态大模型 llava gradio demo</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from lmdeploy import pipeline</span><br><span class="line">from lmdeploy.vl import load_image</span><br><span class="line"></span><br><span class="line">pipe = pipeline(&#x27;liuhaotian/llava-v1.6-vicuna-7b&#x27;)</span><br><span class="line">image = load_image(&#x27;https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/tests/data/tiger.jpeg&#x27;)</span><br><span class="line">response = pipe((&#x27;describe this image&#x27;, image))</span><br><span class="line">print(response)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">(lmdeploy) D:\project\ai_chat\lmdeploy&gt;python pipeline_llava.py</span><br><span class="line">Fetching 13 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00&lt;?, ?it/s]</span><br><span class="line">model_source: hf_model</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;D:\project\ai_chat\lmdeploy\pipeline_llava.py&quot;, line 5, in &lt;module&gt;</span><br><span class="line">    pipe = pipeline(&#x27;liuhaotian/llava-v1.6-vicuna-7b&#x27;)</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\lib\site-packages\lmdeploy\api.py&quot;, line 61, in pipeline</span><br><span class="line">    return AsyncEngine(model_path,</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\lib\site-packages\lmdeploy\serve\async_engine.py&quot;, line 67, in __init__</span><br><span class="line">    self._build_turbomind(model_path=model_path,</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\lib\site-packages\lmdeploy\serve\async_engine.py&quot;, line 108, in _build_turbomind</span><br><span class="line">    self.engine = tm.TurboMind.from_pretrained(</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\lib\site-packages\lmdeploy\turbomind\turbomind.py&quot;, line 438, in from_pretrained</span><br><span class="line">    return cls(model_path=local_path,</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\lib\site-packages\lmdeploy\turbomind\turbomind.py&quot;, line 196, in __init__</span><br><span class="line">    self.model_comm = self._from_hf(model_source=model_source,</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\lib\site-packages\lmdeploy\turbomind\turbomind.py&quot;, line 288, in _from_hf</span><br><span class="line">    model_arch = get_model_from_config(model_path)</span><br><span class="line">  File &quot;D:\develop\miniconda\envs\lmdeploy\lib\site-packages\lmdeploy\turbomind\utils.py&quot;, line 148, in get_model_from_config</span><br><span class="line">    return ARCH_MAP[arch]</span><br><span class="line">KeyError: &#x27;LlavaLlamaForCausalLM&#x27;</span><br></pre></td></tr></table></figure>

<p>报错，做不了</p>
<h2 id="五"><a href="#五" class="headerlink" title="五"></a>五</h2><p>将 LMDeploy Web Demo 部署到 <a target="_blank" rel="noopener" href="https://github.com/InternLM/Tutorial/blob/camp2/tools/openxlab-deploy">OpenXLab</a></p>
<p>SKIP</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/InternLM-3/" rel="prev" title="书生·浦语大模型实战营三">
      <i class="fa fa-chevron-left"></i> 书生·浦语大模型实战营三
    </a></div>
      <div class="post-nav-item">
    <a href="/InternLM-4/" rel="next" title="书生·浦语大模型实战营四">
      书生·浦语大模型实战营四 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%88%9D%E4%BD%93%E9%AA%8C"><span class="nav-number">1.</span> <span class="nav-text">初体验</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%80%9F%E5%BA%A6%E5%AF%B9%E6%AF%94"><span class="nav-number">2.</span> <span class="nav-text">速度对比</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%88%E7%AB%AF%E8%81%8A%E5%A4%A9"><span class="nav-number">3.</span> <span class="nav-text">终端聊天</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8api%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="nav-number">4.</span> <span class="nav-text">启动api服务器</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BD%91%E9%A1%B5%E8%81%8A%E5%A4%A9"><span class="nav-number">5.</span> <span class="nav-text">网页聊天</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#api"><span class="nav-number">5.1.</span> <span class="nav-text">api</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#web"><span class="nav-number">5.2.</span> <span class="nav-text">web</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%BF%9B%E9%98%B6%E4%BD%9C%E4%B8%9A"><span class="nav-number">6.</span> <span class="nav-text">进阶作业</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80"><span class="nav-number">6.1.</span> <span class="nav-text">一</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%8F%E5%8C%96%E6%A8%A1%E5%9E%8B"><span class="nav-number">6.1.1.</span> <span class="nav-text">量化模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%88%E7%AB%AF%E8%BF%90%E8%A1%8C%E9%87%8F%E5%8C%96%E6%A8%A1%E5%9E%8B"><span class="nav-number">6.1.2.</span> <span class="nav-text">终端运行量化模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C"><span class="nav-number">6.2.</span> <span class="nav-text">二</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89"><span class="nav-number">6.3.</span> <span class="nav-text">三</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B"><span class="nav-number">6.4.</span> <span class="nav-text">四</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94"><span class="nav-number">6.5.</span> <span class="nav-text">五</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">秋月</p>
  <div class="site-description" itemprop="description">记录自己的学习经验</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">秋月</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
